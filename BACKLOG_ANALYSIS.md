# Comprehensive Backlog Analysis & Task Breakdown

## Executive Summary

I've conducted deep research using 5 specialized exploration agents to analyze the entire codebase against the requirements in tasks.md. This analysis covered:

1. **Figma Plugin Architecture** (13,000+ LOC validation code exists, NO plugin yet)
2. **Benchmark System** (387 benchmarks running, but 2 critical bugs blocking accuracy)
3. **Specialist Templates** (8 v2 personas complete, inheritance system working)
4. **Testing Infrastructure** (Strong fixtures, but missing automation)
5. **Code Generation Pipeline** (End-to-end working, 85%+ accuracy, <10s latency)

**Result:** Created **15 comprehensive tasks** (task-44 through task-53) covering all gaps identified in tasks.md and current system issues.

---

## Tasks Created Summary

### Parent Task: Third Pass Implementation (task-44)
**Setup third_pass workspace with pnpm monorepo structure**
- Sets up the foundation for all third_pass work
- Creates workspace structure with proper TypeScript config
- 10 acceptance criteria

### Subtasks: Core Functionality (task-44.1 - 44.4)

**task-44.1: Build Zephyr Figma plugin - Extract as Image feature**
- Implements PNG/SVG/JPEG export using figma.exportAsync()
- Includes metadata (version, editDate, nodeId)
- 12 acceptance criteria
- **Priority: HIGH**

**task-44.2: Build Zephyr Figma plugin - Generate as Code (JSON Export) feature**
- Recursive node traversal extracting complete state
- JSON format matching validation pipeline expectations
- Version tracking for change detection
- 14 acceptance criteria
- **Priority: HIGH**

**task-44.3: Integrate validation pipeline into third_pass**
- Package existing validation/ code (13,000+ LOC) as npm package
- Create CLI interface for batch processing
- End-to-end: JSON â†’ Parse â†’ Classify â†’ Map â†’ Generate â†’ Validate
- 14 acceptance criteria
- **Priority: HIGH**

**task-44.4: Build UI demo app with Tanstack Start, Tailwind 4, ShadCN 3**
- Full-stack React framework (not just Router like current new-result-testing)
- Component showcase, visual diff, metrics dashboard
- Live code editing and search
- 15 acceptance criteria
- **Priority: HIGH**

### Critical Bug Fixes (task-45 - 48)

**task-45: Fix score parsing for Claude models in benchmark-runner**
- Claude models return 0 scores despite 100% success
- gpt-4o works correctly (8.93/10)
- Regex patterns need fixing
- 11 acceptance criteria
- **Priority: HIGH - CRITICAL BUG**

**task-46: Integrate prompt combination system into benchmark execution**
- System is built but commented out (parallel-runner.ts:454-492)
- Specialists don't receive full context (persona, docs, model prompts)
- Benchmarks test tier prompts in isolation
- 11 acceptance criteria
- **Priority: HIGH - CRITICAL MISSING FEATURE**

**task-47: Complete figma-extract scenario fixtures and oracle answers**
- Tasks 38-43 partially complete but not integrated
- Fixtures exist but scenarios don't use them
- Need full fixture integration and validation
- 13 acceptance criteria
- **Priority: MEDIUM**

**task-48: Optimize benchmark-runner parallel execution performance**
- Current: 3.8 hours for 387 benchmarks
- Target: <2 hours (50% improvement)
- Add caching, parallelize models, incremental runs
- 12 acceptance criteria
- **Priority: MEDIUM**

### Integration & Polish (task-49 - 53)

**task-49: Create end-to-end workflow connecting plugin â†’ pipeline â†’ UI demo**
- REST API for plugin communication
- WebSocket for real-time updates
- Complete designer workflow: Figma â†’ JSON â†’ Generation â†’ UI display
- 13 acceptance criteria
- **Priority: HIGH**

**task-50: Improve visual similarity scoring to reach 85% target**
- Current: 56% pixel similarity
- Target: 85%
- Fix dimension normalization, use SSIM, better fonts
- 12 acceptance criteria
- **Priority: MEDIUM**

**task-51: Build comprehensive documentation and getting started guide**
- Architecture diagrams, setup guides, API reference
- User guide for designers, developer guide
- Troubleshooting and contribution guidelines
- 15 acceptance criteria
- **Priority: MEDIUM**

**task-52: Implement automated testing and CI/CD pipeline**
- Unit, integration, E2E, visual regression tests
- GitHub Actions for PR checks and deployment
- 80% coverage target
- 15 acceptance criteria
- **Priority: MEDIUM**

**task-53: Create metrics dashboard for tracking generation quality and performance**
- Quality, performance, cost, and trend metrics
- Real-time dashboard with historical analysis
- Export and alerting capabilities
- 14 acceptance criteria
- **Priority: LOW**

---

## Current State Analysis

### âœ… What's Working Exceptionally Well

1. **Code Generation Pipeline** (validation/)
   - 13,000+ LOC, 106 TypeScript files
   - 100% style extraction coverage
   - 85%+ component matching accuracy
   - 3-7 second generation latency
   - $0.013-0.029 cost per component

2. **Specialist Template System** (personas/)
   - 8 v2 specialists with proper inheritance
   - TSConfig-style template loading
   - Prompt combination system built (just needs integration)
   - Comprehensive domain coverage

3. **Benchmark Infrastructure** (specialist_work/packages/benchmark-runner/)
   - Sophisticated parallel execution
   - Template loading with validation
   - Result aggregation and reporting
   - 387 benchmarks running (99.74% success rate)

4. **Fixture System** (ze-benchmarks/suites/figma-extract/fixtures/)
   - 5 committed Figma JSON files
   - 4 mocked API responses
   - Well-documented, deterministic testing

### âŒ Critical Issues Blocking Progress

1. **Score Parsing Bug (task-45)**
   - Claude models return 0 scores
   - Can't accurately measure specialist performance
   - **Impact:** Benchmark results unreliable

2. **Prompt Injection Disabled (task-46)**
   - Specialists run without full context
   - Missing persona, documentation, model-specific prompts
   - **Impact:** Not testing actual specialist templates

3. **No Figma Plugin Exists (tasks 44.1, 44.2)**
   - REST API extraction exists, but no UI in Figma
   - Designers can't use the workflow
   - **Impact:** No designer adoption possible

4. **Visual Similarity Below Target (task-50)**
   - 56% vs 85% target
   - Dimension normalization issues
   - **Impact:** Generated components don't match pixel-perfect

### ðŸ”¶ Gaps vs tasks.md Requirements

| Requirement | Status | Task |
|-------------|--------|------|
| third_pass/ directory with pnpm workspace | âŒ Missing | task-44 |
| Zephyr Figma plugin | âŒ Missing | task-44.1, 44.2 |
| Extract as Image feature | âŒ Missing | task-44.1 |
| Generate as Code (JSON export) | âŒ Missing | task-44.2 |
| Validation pipeline integration | âš ï¸ Code exists, not packaged | task-44.3 |
| UI demo app with Tanstack Start | âš ï¸ Has Router, needs Start | task-44.4 |

---

## Key Questions & Clarifications Needed

### 1. **Figma Plugin Publishing Strategy**
- **Question:** Will the Zephyr Figma plugin be published to the Figma Community?
- **Options:**
  - Private plugin for internal use only
  - Public community plugin (requires review process)
  - Development-only plugin (no publishing)
- **Impact:** Affects manifest configuration and feature scope

### 2. **Tanstack Start vs Tanstack Router**
- **Question:** Do you specifically need Tanstack Start (full-stack SSR framework)?
- **Context:** Current new-result-testing uses Tanstack Router (client-side)
- **Consideration:** Tanstack Start adds SSR complexity - is it required for your use case?
- **Alternative:** Could extend existing new-result-testing with Tanstack Router

### 3. **Benchmark System Priority**
- **Question:** Should I fix the benchmark bugs (tasks 45-46) BEFORE building third_pass?
- **Recommendation:** Fix bugs first to have accurate measurement of specialist templates
- **Rationale:** Need working benchmarks to validate that generated code quality improves

### 4. **Visual Similarity Target**
- **Question:** Is 85% visual similarity truly required, or is 70-80% semantic accuracy sufficient?
- **Context:** Pixel-perfect matching may be too strict for functional equivalence
- **Trade-off:** Semantic accuracy (structure, behavior) vs visual accuracy (exact pixels)

### 5. **API Keys and Service Accounts**
- **Question:** Do you have API keys for all required services?
- **Required:**
  - OpenRouter API key (for Claude Sonnet 4.5 + GPT-4o)
  - Anthropic API key (for direct Claude access)
  - Figma API token (for REST API access)
- **Optional:**
  - OpenAI API key (for embeddings, vision)

### 6. **Deployment Infrastructure**
- **Question:** Where should the UI demo app and validation pipeline API be deployed?
- **Options:**
  - Vercel (recommended for Tanstack Start)
  - Netlify
  - Railway / Render
  - Self-hosted VPS
- **Requirements:** Need to support WebSocket for real-time updates (affects hosting choice)

### 7. **Component Library Scope**
- **Question:** Should the system support only ShadCN v3, or multiple libraries?
- **Context:** Research shows semantic-mapping supports ShadCN, Radix, MUI, Chakra, Ant Design
- **Current:** Focused on ShadCN + Tailwind
- **Future:** Multi-library support?

### 8. **Change Detection Strategy**
- **Question:** How should the system detect when Figma components are updated?
- **Options:**
  - Manual re-export when designer makes changes
  - Webhook integration (Figma notifies on changes)
  - Polling (periodic checks for file modifications)
  - Version comparison (lastModified timestamp)
- **Impact:** Affects plugin architecture and pipeline integration

---

## Recommended Execution Order

### Phase 1: Fix Critical Bugs (Week 1)
**Goal:** Get benchmark system working accurately

1. **task-45** - Fix score parsing for Claude models (1 day)
2. **task-46** - Integrate prompt combination system (1 day)
3. Validate benchmark results with fixed system (1 day)
4. **task-47** - Complete fixture integration (2 days)

**Outcome:** Accurate benchmark measurement of specialist templates

### Phase 2: Build Third Pass Core (Weeks 2-3)
**Goal:** Implement the essential workflow from tasks.md

1. **task-44** - Setup third_pass workspace (1 day)
2. **task-44.2** - Build JSON export feature for plugin (3 days)
3. **task-44.3** - Package validation pipeline (3 days)
4. **task-44.1** - Build image export feature for plugin (2 days)
5. **task-44.4** - Build UI demo app (4 days)

**Outcome:** Working Figma plugin + validation pipeline + demo app

### Phase 3: Integration & Polish (Week 4)
**Goal:** Connect all components and improve quality

1. **task-49** - End-to-end workflow integration (3 days)
2. **task-50** - Improve visual similarity scoring (2 days)
3. **task-51** - Write comprehensive documentation (2 days)

**Outcome:** Complete, documented, integrated system

### Phase 4: Production Readiness (Week 5)
**Goal:** Make it production-ready and maintainable

1. **task-52** - Automated testing and CI/CD (3 days)
2. **task-48** - Optimize benchmark performance (2 days)
3. **task-53** - Metrics dashboard (2 days)

**Outcome:** Production-ready system with monitoring and automation

---

## Technical Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     FIGMA DESIGN FILE                        â”‚
â”‚  (Zephyr Cloud ShadCN Design System)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ZEPHYR FIGMA PLUGIN (NEW)                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚ Extract as Image â”‚  â”‚ Generate as Code â”‚                 â”‚
â”‚  â”‚ PNG/SVG/JPEG     â”‚  â”‚ JSON Export      â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                    â”‚
            â”‚ Images             â”‚ JSON
            â”‚                    â”‚
            â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           VALIDATION PIPELINE (third_pass/packages/)         â”‚
â”‚                                                              â”‚
â”‚  Parse â†’ Classify â†’ Semantic Map â†’ Match Library            â”‚
â”‚    â†“                                                         â”‚
â”‚  Generate Code (Claude 4.5) â†’ Visual Validate â†’ Output      â”‚
â”‚                                                              â”‚
â”‚  Outputs: React/TS Component + Metadata + Validation Report â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        UI DEMO APP (Tanstack Start + Tailwind 4)            â”‚
â”‚                                                              â”‚
â”‚  â€¢ Component Showcase                                        â”‚
â”‚  â€¢ Visual Diff (Figma vs Generated)                         â”‚
â”‚  â€¢ Metrics Dashboard                                         â”‚
â”‚  â€¢ Live Code Editor                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         BENCHMARK SYSTEM (specialist_work/packages/)         â”‚
â”‚                                                              â”‚
â”‚  Specialist Templates â†’ Benchmark Scenarios â†’ Results        â”‚
â”‚                                                              â”‚
â”‚  Measures: Quality, Performance, Cost                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Budget & Performance Projections

### Cost Analysis
**Per Component Generation:**
- Embedding: $0.00003
- Code Generation: $0.005-0.020
- Visual Validation: $0.0085 (optional)
- **Total: ~$0.013-0.029 per component**

**Monthly Projection (300 components):**
- Optimized with caching: $0.70-1.00/month
- Without caching: $2.74/month
- **Savings: 73% with caching**

**Budget Runway on $50 budget:**
- 4-6 years at current usage rate

### Performance Targets
**End-to-End Latency:**
- Simple components: <15s (current: ~5s) âœ…
- Complex components: <30s (current: <10s) âœ…

**Benchmark Execution:**
- Current: 3.8 hours for 387 benchmarks
- Target: <2 hours (task-48)
- Improvement: 50% faster

**Visual Similarity:**
- Current: 56%
- Target: 85% (task-50)
- Gap: 29 percentage points

---

## Success Metrics

### Quality Metrics
- âœ… Component classification: 83-92% accuracy
- âœ… Semantic mapping: 88.4% average confidence
- âœ… Component matching: 85%+ accuracy
- âš ï¸ Visual similarity: 56% (target 85%)

### Performance Metrics
- âœ… Extraction: 37-75ms (binary) / 200-500ms (API)
- âœ… Code generation: 3-7 seconds
- âœ… End-to-end: <10s for complex components
- âš ï¸ Benchmark suite: 3.8 hours (target <2 hours)

### Coverage Metrics
- âœ… 14 component types classified
- âœ… 48 component types in v2 specialist
- âœ… 100% style extraction coverage
- âœ… 106 TypeScript test files

---

## Risk Assessment

### High Risk
1. **Plugin Development Complexity**
   - Risk: Figma Plugin API limitations or bugs
   - Mitigation: Use reference plugins (esbuild-react, codegen) as templates
   - Impact: Could delay tasks 44.1 and 44.2

2. **Score Parsing Bug Root Cause**
   - Risk: Deeper issue than regex patterns
   - Mitigation: Capture full ze-benchmarks output for analysis
   - Impact: Could invalidate all benchmark results

### Medium Risk
1. **Tanstack Start Learning Curve**
   - Risk: Team unfamiliar with SSR framework
   - Mitigation: Consider using Tanstack Router instead (already familiar)
   - Impact: Could add 1-2 weeks to task 44.4

2. **Visual Similarity Target**
   - Risk: 85% may not be achievable with current approach
   - Mitigation: Re-evaluate whether semantic similarity is sufficient
   - Impact: May need to adjust acceptance criteria

### Low Risk
1. **API Rate Limits**
   - Risk: Heavy benchmark usage could hit rate limits
   - Mitigation: Already have concurrency controls and backoff
   - Impact: Minimal, system handles this well

---

## Conclusion

The project has **excellent technical foundations** with sophisticated code generation, semantic mapping, and benchmarking systems. However, **three critical gaps** prevent it from being a complete solution:

1. **No Figma plugin** - designers can't access the workflow
2. **Benchmark bugs** - can't accurately measure improvements
3. **Missing integration** - components exist but don't communicate

The **15 tasks created** comprehensively address all gaps identified in tasks.md and system issues discovered through deep research.

**Recommended next step:** Answer the 8 questions above, then proceed with **Phase 1: Fix Critical Bugs** to establish accurate measurement before building new functionality.

Total estimated effort: **4-5 weeks** for complete implementation through Phase 4.

---

## Next Steps

1. **Review this analysis** and provide feedback
2. **Answer the 8 questions** in the "Key Questions & Clarifications Needed" section
3. **Approve the task breakdown** or request modifications
4. **Choose execution order** (recommended: Phase 1 â†’ 2 â†’ 3 â†’ 4)
5. **Begin implementation** with first approved task

I'm ready to start implementation as soon as you provide clarifications and approval!
